<html>
<head>
<title>main.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
main.py</font>
</center></td></tr></table>
<pre><span class="s0">#################</span>
<span class="s0">##### LAB 4 #####</span>
<span class="s0">#################</span>

<span class="s0">#################</span>
<span class="s0"># Import libraries</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span>
<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">decomposition</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">confusion_matrix</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">ConfusionMatrixDisplay</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.neighbors </span><span class="s2">import </span><span class="s1">KNeighborsClassifier</span>
<span class="s0">#################</span>

<span class="s0">#################################################################</span>
<span class="s0">##### Exercise 1 : PCA through Singular Value Decomposition #####</span>
<span class="s0">#################################################################</span>

<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Exercise 1 : PCA through Singular Value Decomposition :</span><span class="s2">\n</span><span class="s3">'</span><span class="s1">)</span>

<span class="s0"># Define 3 points matrix in 2D-space and its transposed matrix:</span>
<span class="s1">X = np.array([[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]])</span>
<span class="s1">Xt = np.transpose(X)</span>

<span class="s0"># Display the matrix</span>
<span class="s1">print(</span><span class="s3">'X =</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">X)</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Xt =</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">Xt)</span>

<span class="s0"># Calculate the covariance matrix:</span>
<span class="s1">R = (</span><span class="s4">1</span><span class="s1">/</span><span class="s4">3</span><span class="s1">)*np.matmul(X</span><span class="s2">, </span><span class="s1">Xt)</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">R =</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">R)</span>

<span class="s0"># Calculate the SVD decomposition and new basis vectors:</span>
<span class="s1">[U</span><span class="s2">, </span><span class="s1">D</span><span class="s2">, </span><span class="s1">V] = np.linalg.svd(R)  </span><span class="s0"># Call SVD decomposition</span>
<span class="s1">u1 = U[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]    </span><span class="s0"># New basis vectors</span>
<span class="s1">u2 = U[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span>

<span class="s0"># Calculate the coordinates in new orthonormal basis:</span>
<span class="s1">Xi1 = np.matmul(Xt</span><span class="s2">, </span><span class="s1">u1)</span>
<span class="s1">Xi2 = np.matmul(Xt</span><span class="s2">, </span><span class="s1">u2)</span>

<span class="s0"># Display the coordinates</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Xi1 ='</span><span class="s2">, </span><span class="s1">Xi1)</span>
<span class="s1">print(</span><span class="s3">'Xi2 ='</span><span class="s2">, </span><span class="s1">Xi2)</span>

<span class="s0"># Calculate the approximation of the original from new basis</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Second matrix dimension :</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">Xi1[:</span><span class="s2">, None</span><span class="s1">])     </span><span class="s0"># Add second dimension to array and test it</span>

<span class="s1">XApprox = np.matmul(u1[:</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xi1[</span><span class="s2">None, </span><span class="s1">:])      </span><span class="s0"># Approximation</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Original approximation :</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">XApprox)</span>

<span class="s0"># Check that you got the original</span>
<span class="s1">XOrigin = np.matmul(u1[:</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xi1[</span><span class="s2">None, </span><span class="s1">:]) + np.matmul(u2[:</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xi2[</span><span class="s2">None, </span><span class="s1">:])</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Original matrix :</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">XOrigin)</span>
<span class="s1">print()</span>
<span class="s0">#################</span>


<span class="s0">#########################################</span>
<span class="s0">##### Exercise 2 : PCA on Iris data #####</span>
<span class="s0">#########################################</span>

<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Exercise 2 : PCA on Iris data :</span><span class="s2">\n</span><span class="s3">'</span><span class="s1">)</span>

<span class="s0"># Load Iris dataset as in the last PC lab:</span>
<span class="s1">iris = load_iris()</span>

<span class="s0"># Get Iris feature names</span>
<span class="s1">print(</span><span class="s3">'Iris feature names :'</span><span class="s1">)</span>
<span class="s1">print(iris.feature_names)</span>

<span class="s0"># Get Iris data between 0 &amp; 5</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Iris data :'</span><span class="s1">)</span>
<span class="s1">print(iris.data[</span><span class="s4">0</span><span class="s1">:</span><span class="s4">5</span><span class="s2">, </span><span class="s1">:])</span>

<span class="s0"># Get Iris target</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Iris target :'</span><span class="s1">)</span>
<span class="s1">print(iris.target[:])</span>

<span class="s0"># We have 4 dimensions of data, plot the first three columns in 3D</span>
<span class="s1">X = iris.data</span>
<span class="s1">y = iris.target</span>

<span class="s0"># Initialize axes</span>
<span class="s1">axes1 = plt.axes(projection=</span><span class="s3">'3d'</span><span class="s1">)</span>
<span class="s1">axes1.scatter3D(X[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'green'</span><span class="s1">)</span>
<span class="s1">axes1.scatter3D(X[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'blue'</span><span class="s1">)</span>
<span class="s1">axes1.scatter3D(X[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">X[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'magenta'</span><span class="s1">)</span>

<span class="s0"># Display the graphic</span>
<span class="s1">plt.show()</span>

<span class="s0"># Pre-processing is an important step, you can try either StandardScaler (zero mean, unit variance of features)</span>
<span class="s0"># or MinMaxScaler (to interval from 0 to 1)</span>
<span class="s1">Xscaler = StandardScaler()</span>
<span class="s1">Xpp = Xscaler.fit_transform(X)</span>

<span class="s0"># Define PCA object (three components), fit and transform the data</span>
<span class="s1">pca = decomposition.PCA(n_components=</span><span class="s4">3</span><span class="s1">)</span>
<span class="s1">pca.fit(Xpp)</span>
<span class="s1">Xpca = pca.transform(Xpp)</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">PCA Covariance :</span><span class="s2">\n</span><span class="s3">'</span><span class="s2">, </span><span class="s1">pca.get_covariance())</span>

<span class="s0"># Initialize axes</span>
<span class="s1">axes2 = plt.axes(projection=</span><span class="s3">'3d'</span><span class="s1">)</span>
<span class="s1">axes2.scatter3D(Xpca[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'green'</span><span class="s1">)</span>
<span class="s1">axes2.scatter3D(Xpca[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'blue'</span><span class="s1">)</span>
<span class="s1">axes2.scatter3D(Xpca[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'magenta'</span><span class="s1">)</span>

<span class="s0"># Display the graphic</span>
<span class="s1">plt.show()</span>

<span class="s0"># Compute pca.explained_variance_ value</span>
<span class="s1">explainedVariance = pca.explained_variance_</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Explained variance ='</span><span class="s2">, </span><span class="s1">explainedVariance)</span>

<span class="s0"># Compute pca.explained_variance_ratio value</span>
<span class="s1">explainedVarianceRatio = pca.explained_variance_ratio_</span>
<span class="s1">print(</span><span class="s3">'Explained variance ratio ='</span><span class="s2">, </span><span class="s1">explainedVarianceRatio)</span>

<span class="s0"># Plot the principal components in 2D, mark different targets in color</span>
<span class="s1">plt.scatter(Xpca[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'green'</span><span class="s1">)</span>
<span class="s1">plt.scatter(Xpca[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'blue'</span><span class="s1">)</span>
<span class="s1">plt.scatter(Xpca[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Xpca[y == </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">color=</span><span class="s3">'magenta'</span><span class="s1">)</span>

<span class="s0"># Display the graphic</span>
<span class="s1">plt.show()</span>
<span class="s1">print()</span>
<span class="s0">#################</span>


<span class="s0">#######################################</span>
<span class="s0">##### Exercise 3 : KNN classifier #####</span>
<span class="s0">#######################################</span>

<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">Exercise 3 : KNN classifier :</span><span class="s2">\n</span><span class="s3">'</span><span class="s1">)</span>

<span class="s0"># Part 1 : Specifies percentage of the data (here 30% of the data will be used for the testing and 70% for the checking)</span>
<span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size=</span><span class="s4">0.3</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">'X_train shape :'</span><span class="s2">, </span><span class="s1">X_train.shape)</span>
<span class="s1">print(</span><span class="s3">'X_test shape :'</span><span class="s2">, </span><span class="s1">X_test.shape)</span>

<span class="s0"># Choose the needed number of neighbors</span>
<span class="s1">knn1 = KNeighborsClassifier(n_neighbors=</span><span class="s4">3</span><span class="s1">)</span>

<span class="s0"># Fit the matrix</span>
<span class="s1">knn1.fit(X_train</span><span class="s2">, </span><span class="s1">y_train)</span>

<span class="s0"># Compute the Ypred</span>
<span class="s1">Ypred = knn1.predict(X_test)</span>

<span class="s0"># Show confusion matrix</span>
<span class="s1">confusion_matrix(y_test</span><span class="s2">, </span><span class="s1">Ypred)</span>
<span class="s1">ConfusionMatrixDisplay.from_predictions(y_test</span><span class="s2">, </span><span class="s1">Ypred)</span>
<span class="s1">plt.show()</span>
<span class="s0">#################</span>

<span class="s0">#################</span>
<span class="s0"># Part 2 : Now do the same (data set split, KNN, confusion matrix),</span>
<span class="s0"># but for PCA-transformed data (1st two principal components, i.e., first two columns).</span>
<span class="s0"># Compare the results with full dataset</span>
<span class="s1">X_train_pca</span><span class="s2">, </span><span class="s1">X_test_pca</span><span class="s2">, </span><span class="s1">y_train_pca</span><span class="s2">, </span><span class="s1">y_test_pca = train_test_split(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size=</span><span class="s4">0.3</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">X_train_PCA shape :'</span><span class="s2">, </span><span class="s1">X_train_pca.shape)</span>
<span class="s1">print(</span><span class="s3">'X_test_PCA shape :'</span><span class="s2">, </span><span class="s1">X_test_pca.shape)</span>

<span class="s0"># Choose the needed number of neighbors</span>
<span class="s1">knn1 = KNeighborsClassifier(n_neighbors=</span><span class="s4">3</span><span class="s1">)</span>

<span class="s0"># Fit the matrix</span>
<span class="s1">knn1.fit(X_train_pca</span><span class="s2">, </span><span class="s1">y_train_pca)</span>

<span class="s0"># Compute the Ypred</span>
<span class="s1">Ypred_pca = knn1.predict(X_test_pca)</span>

<span class="s0"># Show confusion matrix</span>
<span class="s1">confusion_matrix(y_test_pca</span><span class="s2">, </span><span class="s1">Ypred_pca)</span>
<span class="s1">ConfusionMatrixDisplay.from_predictions(y_test_pca</span><span class="s2">, </span><span class="s1">Ypred_pca)</span>
<span class="s1">plt.show()</span>
<span class="s0">#################</span>

<span class="s0">#################</span>
<span class="s0"># Part 3 : Now do the same, but use only 2-dimensional data of original X (first two columns)</span>
<span class="s1">X_train_wrong</span><span class="s2">, </span><span class="s1">X_test_wrong</span><span class="s2">, </span><span class="s1">y_train_wrong</span><span class="s2">, </span><span class="s1">y_test_wrong = train_test_split(X[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">:</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size=</span><span class="s4">0.3</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">'</span><span class="s2">\n</span><span class="s3">X_train_PCA shape :'</span><span class="s2">, </span><span class="s1">X_train_pca.shape)</span>
<span class="s1">print(</span><span class="s3">'X_test_PCA shape :'</span><span class="s2">, </span><span class="s1">X_test_pca.shape)</span>

<span class="s0"># Choose the needed number of neighbors</span>
<span class="s1">knn1 = KNeighborsClassifier(n_neighbors=</span><span class="s4">3</span><span class="s1">)</span>

<span class="s0"># Fit the matrix</span>
<span class="s1">knn1.fit(X_train_wrong</span><span class="s2">, </span><span class="s1">y_train_wrong)</span>

<span class="s0"># Compute the Ypred</span>
<span class="s1">Ypred_wrong = knn1.predict(X_test_wrong)</span>

<span class="s0"># Show confusion matrix</span>
<span class="s1">confusion_matrix(y_test_wrong</span><span class="s2">, </span><span class="s1">Ypred_wrong)</span>
<span class="s1">ConfusionMatrixDisplay.from_predictions(y_test_wrong</span><span class="s2">, </span><span class="s1">Ypred_wrong)</span>
<span class="s1">plt.show()</span>
<span class="s0">#################</span>
</pre>
</body>
</html>